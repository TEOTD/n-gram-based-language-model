import logging
import typing
from collections import Counter, defaultdict

import numpy as np

smoothing_value = 1e-8
train_path = "A1_DATASET/train.txt"
val_path = "A1_DATASET/val.txt"

# Set logging / print options
logging.basicConfig(level=logging.ERROR)
logger = logging.getLogger(__name__)
np.set_printoptions(precision=3)

def preprocess_line(line: str, dict: set[str] = None) -> typing.List[str]:
    """
    Turns a line string into a list of tokens. Adds <start> and <end> to the start and end.

    Inputs:
        line: The input line string.
        dict: Optional set of known tokens. Unknown tokens will be replaced with <unk>.

    Returns:
        The preprocessed list of tokens.
    """
    l = line.strip().split()
    l.insert(0, '<start>')
    l.append('<end>')

    if dict is not None:
        l = [token if token in dict else '<unk>' for token in l]
    return l

with open(train_path, "r") as f:
    train_corpus = list(map(preprocess_line, f.readlines()))

token_dict = {'<start>', '<end>', '<unk>'}
for line in train_corpus:
    for token in line:
        token_dict.add(token)

# Unigram model

word_count = sum(len(line) for line in train_corpus)

unigram_freq = Counter()
for line in train_corpus:
    for token in line:
        unigram_freq[token] += 1

unigram_probs = {token: count / word_count for token, count in unigram_freq.items()}
tokens = list(unigram_probs.keys())
freqs = list(unigram_probs.values())

for i in unigram_probs:
    logger.info(f"Unigram '{i}': {unigram_probs[i]}")

laplace_unigram_probs = {token: (unigram_freq.get(token, 0) + 1) / (word_count + len(token_dict))
                 for token in token_dict}

for i in laplace_unigram_probs:
    logger.info(f"Unigram Laplace '{i}': {laplace_unigram_probs[i]}")

k = 0.5
addk_unigram_probs = {token: (unigram_freq.get(token, 0) + k) / (word_count + k * len(token_dict))
              for token in token_dict}

for i in addk_unigram_probs:
    logger.info(f"Unigram Addk '{i}': {addk_unigram_probs[i]}")

def unigram_model(line: typing.List[str], probs: dict[str, float]) -> float:
    """
    Returns the probability of a random line generated by the unigram model.

    Inputs:
        line: A list of tokens representing the line.
        probs: A dictionary mapping tokens to their probabilities.
    
    Returns:
        A float representing the natural log of the probability of the line.
    """
    prob = 0
    for token in line:
        prob += np.log(probs.get(token, smoothing_value))
    return prob

# Bigram model

bigram_freq = defaultdict(Counter)
for line in train_corpus:
    for i in range(len(line)-1):
        bigram_freq[line[i]][line[i+1]] += 1

bigram_probs = {}
for token, counter in bigram_freq.items():
    total_count = sum(counter.values())
    bigram_probs[token] = {t: c / total_count for t, c in counter.items()}

for i in bigram_probs:
    logger.info(f"Bigram '{i}': {bigram_probs[i]}")

# Exclude <start> from continuation token dict as <start> only appears as the first element of a bigram.
# It never appears as the second element, since no token precedes <start>.
# Hence, when calculating probabilities of bigram, we can safely exclude <start> from the denominatorâ€™s V, because it will never be predicted.
continuation_token_dict = token_dict - {'<start>'}
V = len(continuation_token_dict)

# Laplace smoothing
laplace_bigram_probs = {}
for w1 in token_dict:
    total_count = sum(bigram_freq[w1].values())
    laplace_bigram_probs[w1] = {}
    for w2 in continuation_token_dict:
        count = bigram_freq[w1].get(w2, 0)
        laplace_bigram_probs[w1][w2] = (count + 1) / (total_count + V)

for i in laplace_bigram_probs:
    logger.info(f"Bigram Laplace '{i}': {laplace_bigram_probs[i]}")

# Add-k smoothing
k = 0.5
addk_bigram_probs = {}
for w1 in token_dict:
    total_count = sum(bigram_freq[w1].values())
    addk_bigram_probs[w1] = {}
    for w2 in continuation_token_dict:
        count = bigram_freq[w1].get(w2, 0)
        addk_bigram_probs[w1][w2] = (count + k) / (total_count + k * V)

for i in addk_bigram_probs:
    logger.info(f"Bigram Addk '{i}': {addk_bigram_probs[i]}")

def bigram_model(line: typing.List[str], probs: dict[str, dict[str, float]]) -> float:
    """
    Returns the probability of a random line generated by the bigram model.

    Input:
        line: A list of tokens representing the line.
        probs: A dictionary mapping tokens to their probabilities.
    
    Output:
        A float representing the natural log of the probability of the line.
    """
    prob = 0
    for i in range(1, len(line)):
        bigram = (line[i-1], line[i])
        prob += np.log(probs.get(bigram[0], {}).get(bigram[1], smoothing_value))
    return prob

# Perplexity

def perplexity(corpus: list[list[str]], model_fn, probs) -> float:
    """
    Compute perplexity for a given corpus under a language model.

    Inputs:
        corpus: List of tokenized lines (each line is a list of tokens).
        model_fn: Function that takes (line, probs) and returns log probability.
        probs: Probability dictionary for the model.

    Returns:
        Perplexity (float).
    """
    total_log_prob = 0.0
    total_tokens = 0
    for line in corpus:
        total_log_prob += model_fn(line, probs)
        total_tokens += len(line)
    avg_log_prob = total_log_prob / total_tokens
    return np.exp(-avg_log_prob)

# Validation tests

with open(val_path, "r") as f:
    val_corpus = []
    for line in f.readlines():
        val_corpus.append(preprocess_line(line, token_dict))
    for line in val_corpus:
        unigram_prob = np.array([unigram_model(line, unigram_probs)])
        laplace_unigram_prob = np.array([unigram_model(line, laplace_unigram_probs)])
        addk_unigram_prob = np.array([unigram_model(line, addk_unigram_probs)])

        bigram_prob = np.array([bigram_model(line, bigram_probs)])
        laplace_bigram_prob = np.array([bigram_model(line, laplace_bigram_probs)])
        addk_bigram_prob = np.array([bigram_model(line, addk_bigram_probs)])

        print (" ".join(line))
        print()
        print (f"Unigram prob: {unigram_prob}")
        print (f"Unigram Laplace prob: {laplace_unigram_prob}")
        print (f"Unigram Addk prob: {addk_unigram_prob}")
        print()
        print (f"Bigram prob: {bigram_prob}")
        print (f"Bigram Laplace prob: {laplace_bigram_prob}")
        print (f"Bigram Addk prob: {addk_bigram_prob}")

    # # Compute perplexities
    # print("\n=== Perplexities on Validation Set ===")
    # print("Unigram:", perplexity(val_corpus, unigram_model, unigram_probs))
    # print("Unigram Laplace:", perplexity(val_corpus, unigram_model, laplace_unigram_probs))
    # print("Unigram Add-k:", perplexity(val_corpus, unigram_model, addk_unigram_probs))
    # print("Bigram:", perplexity(val_corpus, bigram_model, bigram_probs))
    # print("Bigram Laplace:", perplexity(val_corpus, bigram_model, laplace_bigram_probs))
    # print("Bigram Add-k:", perplexity(val_corpus, bigram_model, addk_bigram_probs))

    # Compute perplexities - table output
    print("\n=== Perplexities on Validation Set ===")
    print(f"{'Model':<20} {'Perplexity':>12}")
    print("-" * 32)
    print(f"{'Unigram':<20} {perplexity(val_corpus, unigram_model, unigram_probs):>12.3f}")
    print(f"{'Unigram Laplace':<20} {perplexity(val_corpus, unigram_model, laplace_unigram_probs):>12.3f}")
    print(f"{'Unigram Add-k':<20} {perplexity(val_corpus, unigram_model, addk_unigram_probs):>12.3f}")
    print(f"{'Bigram':<20} {perplexity(val_corpus, bigram_model, bigram_probs):>12.3f}")
    print(f"{'Bigram Laplace':<20} {perplexity(val_corpus, bigram_model, laplace_bigram_probs):>12.3f}")
    print(f"{'Bigram Add-k':<20} {perplexity(val_corpus, bigram_model, addk_bigram_probs):>12.3f}")

